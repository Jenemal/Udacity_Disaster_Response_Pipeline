{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# download NLTK data\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load in the data from the database\n",
    "def load_database_data():\n",
    "\n",
    "    engine = create_engine('sqlite:///UdacityProject2.db')\n",
    "    df = pd.read_sql_table('DisasterResponseClean', engine)\n",
    "\n",
    "    X = df.message\n",
    "    Y = df.drop(['message', 'original', 'id', 'genre'], axis = 1)\n",
    "    target_names = Y.columns\n",
    "\n",
    "    return X, Y, target_names\n",
    "\n",
    "X, Y, category_names = load_database_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    # initialize WordNetLemmatizer   \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # replace URLs with blanks\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "   \n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    # replace each url in text string with blanks\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"\")\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tok = re.sub(r\"[^a-zA-Z0-9]\", \" \", clean_tok)\n",
    "        clean_tokens.append(clean_tok)  \n",
    "    \n",
    "    # Remove stop words\n",
    "    final_clean_tokens = [w for w in clean_tokens if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    return final_clean_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the model using a machine learning pipeline\n",
    "def build_pipeline():\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('vector',CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, Y_test, category_names):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report  = classification_report(Y_test, Y_pred, target_names=category_names)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Loading the data from the database:')\n",
    "    start = time.time()\n",
    "    X, Y, target_names = load_database_data()\n",
    "    end = time.time()\n",
    "    print('Data successfully loaded! Runtime (sec): ', (end - start))\n",
    "\n",
    "    print('Split test-train:')\n",
    "    start = time.time()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "    end = time.time()\n",
    "    print('Test-train split successful! Runtime (sec): ', (end - start))\n",
    "\n",
    "    print('Build pipeline:')\n",
    "    start = time.time()\n",
    "    pipeline = build_pipeline()\n",
    "    end = time.time()\n",
    "    print('Build successful! Runtime: (sec)', (end - start))\n",
    "\n",
    "    print('Train the model:')\n",
    "    start = time.time()\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    end = time.time()\n",
    "    print('Training successful! Runtime: (min)', (end - start)/60)\n",
    "\n",
    "    print('Testing the model and generating report:')\n",
    "    test_model(pipeline, X_test, Y_test, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
